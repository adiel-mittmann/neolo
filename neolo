#!/usr/bin/env python3
#
# script commissioned by Saulo
# intended to identify "neologisms" in a text by
# searching through a given dictionary/wordlist/othertext
#
# we also compute various statistics on the text and include them in the result

# arg1 is the text to be analysed
# arg2 .. argN are texts to be compared against

import sys
import re
from collections import Counter
from math import factorial as fact
from math import sqrt
from functools import reduce
import operator
import argparse
import codecs

PUNC_RE = re.compile('[_+\-\(\)\.,:;!?\'"]')

# porter stemmer

def usage():
  """Print a helpful usage message."""
  print("Usage:\nneolo TEXT DICT1 [DICT2 ...]")

def lemma1(N,p):
  """return a value for p which is the sum of int(n/p^i) for all i where
  int(n/p^i) <= n"""

  v = 1
  i = 1
  tosum = []
  while v>=1:
    v = int(float(N)/float(p**i))
    if v<1:
      break
    tosum.append(v)
    i+=1
  return sum(tosum)

def lemma2(X):
  """See Wu 1993'An Accurate computation of the hypergeometric
  distribution function'.

  We can calculate n! = p1^r1*p2^r2...pk^rk where ps are prime numbers less
  than or equal to n.  The exponent for each p is given by lemma1(n,p).

  So, we will return here for X! a list of primes to be multipled.  For example,
  X = 5.

  1. Find primes less than or equal to 5
    erasthos(5) = [2, 3, 5]
  2. For each of these primes, find the exponent
    2: int(5/2) + int(5/2^2) = 2 + 1 = 3
    3: int(5/3) = 1
    5: int(5/5) = 1

  3. we have [ 2^3 * 3^1 * 5^1 ] so return {2:3,3:1,5:1}"""

  primes = erathos(X)
  exp = {}
  for p in primes:
    exp[p] = lemma1(X,p)
  return exp


def erathos(N):
  """Return all prime numbers <= N"""
  if N < 2: return []
  lng = ((N//2)-1+N%2)
  sieve = [True]*(lng+1)
  for i in range(int(sqrt(N)) >> 1):
    if not sieve[i]: continue
    for j in range((i*(i+3)<<1) +3, lng, (i<<1)+3):
      sieve[j] = False
  primes = [2]
  primes.extend([(i<<1) + 3 for i in range(lng) if sieve[i]])
  return primes

def wu_hypergeom_0(r,n,N):
  """Instead of using m!(N-r)!/N!(m-r)!, caclulate a series of factors
  for num and denom and then cancel common terms for a simplest fraction
  to do the actual computation"""

  m = N - n
  m_fact = lemma2(m)
  N_minus_r_fact = lemma2(N-r)
  N_fact = lemma2(N)
  m_minus_r_fact = lemma2(m-r)

  # numerator
  num = Counter(m_fact)
  num.update(N_minus_r_fact)

  # denominator
  den = Counter(N_fact)
  den.update(m_minus_r_fact)

  # now reduce the faction to simplest terms
  simple_num = {}
  simple_den = {}
  for f in num:
    if f in den:
      if num[f] == den[f]: continue
      elif num[f] < den[f]: simple_den[f] = den[f]-num[f]
      else: simple_num[f] = num[f] - den[f]
    else: simple_num[f] = num[f]
  for f in [ v for v in den if v not in num ]:
    simple_den[f] = den[f]


  # finally, evaluate the fraction
  final_num = product([k**v for k,v in simple_num.items()])
  final_den = product([k**v for k,v in simple_den.items()])

  return final_num/final_den

def product(iterable):
  return reduce(operator.mul, iterable, 1)

def hypergeom_0(r,n,N):
  getcontext().prec = 3
  m = N - n
  return Decimal(fact(m)*fact(N-r))/Decimal(fact(N)*fact(m-r))

def tokenize(f):
  """Tokenize all the lines in a file"""
  # TODO need a more robust, crosslingual tokenizer
  return [PUNC_RE.sub(" ",l) for l in f]

def stem(f):
  """Stem, downcase all the lines in a file"""
  # TODO: still need a stem method (this is language dependent!
  return [l.lower() for l in f]

def count(f):
  """Return the words of f as a dictionary with keys as wordforms and
  counts as values"""
  return Counter(f)

def chunks(l, n):
  """Yield successive n-sized chunks from l"""
  for i in range(0, len(l), n):
    yield l[i:i+n]

def hdd(types):
  """Calculate HD-D (McCarthy and Jarvis 2007,2010),

  1. for each wordtype, calculate the probability of drawing it
  in a random sample of 42 tokens
  (note, the positive occurance of a wordtype = 1/42 of the TTR, because
  subsequent appearances of that type in a sample don't add another type)

  2. need to consider probability of occuring once + twice + ...
  therefore it's easier to consider the probability of occuring 0 times and
  take 1 - p.


  3. calculate this values * 1/42 to get each word's contribution

  main hypergeom function:
  h(x;r,n,N) = n!r!(N-n)!(N-r)! / N!x!(n-x)!(r-x)!(N-n-r+x)!
  but see Wu 1993 for a simplified method
  """

  hdd = 0.0
  toks = sum(types.values())
  for t in types:
    hdd += (1-wu_hypergeom_0(42, types[t], toks))*(1/42)
  return hdd

def msttr(text):
  """Mean segmental type-token ratio (Johnson,1944)

  From Saulo's description:
  1. Divide the text into equal segments by word tokens (100 words)
  2. For each segment, calculate the token-type ratio
  3. Calculate the arithmetic mean of the TTRs for each segment
  """

  segments = chunks(" ".join(text).split(),100)
  ttrs=[]
  for seg in segments:
    types = Counter(seg)
    ttrs.append(float(len(types))/float(sum(types.values())))
  return sum(ttrs)/float(len(ttrs))

def mltd(text):
  """Measure of Lexical Textual Diversity (McCarthy, 2005)

  From Saulo's description:
  1. Start by adding words to a segment until type/token ratio falls to 0.72,
  then continue at the next segment
  2. Measure ratio of tokens/number-of-segments"""
  seg_n = 0
  seg = []
  toks = 0
  for w in " ".join(text).split():
    toks+=1
    seg.append(w)
    types = Counter(seg)
    if float(len(types))/float(sum(types.values()))<0.72:
      seg_n+=1
      seg = []
  return float(toks)/float(seg_n)

def setup_argparse():
  ap = argparse.ArgumentParser(
      description="Extract lexical statistics from a text file.",
      epilog="Select one or more statistics via the above options.")
  ap.add_argument('text', metavar="TEXT", type=str,
      help="the text you want to investigate")
  ap.add_argument('--dicts', metavar="DICT", nargs='+',
      help="a list of reference texts to compute neologism count")
  ap.add_argument('--mltd', action="store_true",
      help="measure of lexical texual diversity")
  ap.add_argument('--msttr', action="store_true",
      help="mean segmental type-token ratio")
  ap.add_argument('--hdd', action="store_true", help="HD-D probabilistic TTR")
  ap.add_argument('--verbose', '-v', action='count', dest='verbosity',
      default=1, help='increase the verbosty (can be repeated: -vvv)')
  return ap

def try_open(filename):
  """Try several encodings and return the lines from the first one 
     that seems to work"""
  text = ''
  encodings = ['utf-8', 'latin1', 'windows-1250', 'windows-1252']
  for e in encodings:
    try:
      fh = codecs.open(filename, 'r', encoding=e)
      text = fh.readlines()
    except UnicodeDecodeError:
      print('WARNING: Got unicode error with %s , trying different encoding' % e,
             file=sys.stderr)
    else:
      print('Opening',filename,'with encoding:  %s ' % e, file=sys.stderr)
      break 
  return text

if __name__ in  "__main__":
  ap = setup_argparse()
  args = ap.parse_args()
  text = try_open(args.text)

  #TODO convert to logging messages, synchronize with verbosity
  print("Tokenizing, downcasing, stemming text:",args.text,"...",end=" ", file=sys.stderr)
  sys.stderr.flush()
  clean_text = stem(tokenize(text))
  print("done.", file=sys.stderr)
  # get the wordtypes and counts
  print("Counting and sorting words in text:",args.text,"...",end="", file=sys.stderr)
  sys.stderr.flush()
  text_words = count([w for l in clean_text for w in l.split()])
  print("done.", file=sys.stderr)
  hapax = [ w for w in text_words if text_words[w] == 1 ]


  dicts = []
  if args.dicts:
    dicts = [ try_open(f) for f in args.dicts ]
    print("Tokenizing, downcasing, stemming dict files:",
          args.dicts,"...",end=" ", file=sys.stderr)
    sys.stderr.flush()
    clean_dicts = [ stem(tokenize(d)) for d in dicts ]
    print("done.", file=sys.stderr)
    print("Counting and sorting words in dictonaries:",args.dicts,"...",
          end="", file=sys.stderr)
    sys.stderr.flush()
    dict_words = count([w for l in
                      [line for clean_d in clean_dicts for line in clean_d]
                      for w in l.split()])
    print("done.", file=sys.stderr)

    # print neolo
    print("Neologism list:")
    neolo = 0
    for w in sorted(text_words):
      if not w in dict_words:
        print(w)
        neolo+=1

  print("\nStatistics:")
  print("-----------")
  tokens = sum(text_words.values())
  print("Text size:",str(tokens),
        "tokens in",str(len(text_words)),"types.")

  ## Saulo's requested default stats
  print("Number of hapax legomena:",len(hapax))      
  print("TTR (type-token ratio):",float(len(text_words))/float(tokens))
  print("HTR (hapax-token ratio):",float(len(hapax))/float(tokens))
  print("HTyR (hapax-type ratio):",float(len(hapax))/float(len(text_words)))

  if args.dicts:
    print("Neologisms: ",str(neolo), "types not found in",
          str(len(clean_dicts)),"dictionaries")
    print("Dictionaries contained",str(sum(dict_words.values())),
          "tokens in",str(len(dict_words)),"types.")


  if args.msttr:
    print("MSTTR...",end="")
    sys.stdout.flush()
    print(msttr(clean_text))
  if args.mltd:
    print("MLTD...",end="")
    sys.stdout.flush()
    print(mltd(clean_text))
  if args.hdd:
    print("HD-D...",end="")
    sys.stdout.flush()
    print(hdd(text_words))
